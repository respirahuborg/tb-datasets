{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RespiraHub — Trial 4: Push Wav2Vec2 to the Limit\n",
    "\n",
    "**Context:** 3 trial dengan variasi preprocessing semua stuck di 0.69-0.73. DREAM Challenge winner: 0.743. Bottleneck = model backbone.\n",
    "\n",
    "**Perubahan dari Trial 1 (best mean 0.733):**\n",
    "1. Unfreeze last 2 transformer layers (deeper fine-tuning)\n",
    "2. Audio augmentation (noise, time shift, pitch shift)\n",
    "3. Epochs: 10 with early stopping (patience 3)\n",
    "4. Segmentasi: Trial 1 style (concatenate all + split 3s) — proven best mean\n",
    "\n",
    "**Target:** Beat DREAM Challenge winner (0.743). This is the final Wav2Vec2 attempt before switching to HeAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Model\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "    print('Using Apple Silicon MPS')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print('Using CUDA')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "print(f'PyTorch {torch.__version__}, Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = '/Users/aida/code/development/tb-datasets/data/solicited/'\n",
    "CLINICAL_PATH = '/Users/aida/code/development/tb-datasets/data/metadata/CODA_TB_Clinical_Meta_Info.csv'\n",
    "ADDITIONAL_PATH = '/Users/aida/code/development/tb-datasets/data/metadata/CODA_TB_additional_variables_train.csv'\n",
    "SOLICITED_PATH = '/Users/aida/code/development/tb-datasets/data/metadata/CODA_TB_Solicited_Meta_Info.csv'\n",
    "\n",
    "clinical = pd.read_csv(CLINICAL_PATH)\n",
    "additional = pd.read_csv(ADDITIONAL_PATH)\n",
    "solicited = pd.read_csv(SOLICITED_PATH)\n",
    "\n",
    "meta = clinical.merge(additional, on='participant', how='left')\n",
    "meta['label'] = meta['tb_status'].astype(int)\n",
    "\n",
    "print(f'Participants: {len(meta)}')\n",
    "print(f'TB+: {meta[\"label\"].sum()}, TB-: {(meta[\"label\"] == 0).sum()}')\n",
    "print(f'Prevalence: {meta[\"label\"].mean():.1%}')\n",
    "\n",
    "df = solicited.merge(meta, on='participant', how='left')\n",
    "df['filepath'] = df['filename'].apply(lambda f: os.path.join(AUDIO_DIR, f))\n",
    "df['file_exists'] = df['filepath'].apply(os.path.exists)\n",
    "df = df[df['file_exists']].reset_index(drop=True)\n",
    "\n",
    "print(f'Total cough files: {len(df)} from {df[\"participant\"].nunique()} participants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Trial 1 Segmentation (Concatenate All + Split 3s)\n",
    "\n",
    "Back to Trial 1 approach — proven best mean AUROC (0.733)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SR = 16000\n",
    "SEGMENT_SEC = 3.0\n",
    "SEGMENT_SAMPLES = int(TARGET_SR * SEGMENT_SEC)  # 48000\n",
    "GAP_SAMPLES = int(TARGET_SR * 0.05)  # 50ms gap\n",
    "\n",
    "def load_audio(filepath, target_sr=16000):\n",
    "    try:\n",
    "        w, sr = torchaudio.load(filepath)\n",
    "        if w.shape[0] > 1:\n",
    "            w = w.mean(dim=0, keepdim=True)\n",
    "        if sr != target_sr:\n",
    "            w = torchaudio.transforms.Resample(sr, target_sr)(w)\n",
    "        return w.squeeze(0)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Group by participant, concatenate all coughs, split into 3s segments\n",
    "grouped = df.groupby('participant').agg(\n",
    "    label=('label', 'first'),\n",
    "    filepaths=('filepath', list),\n",
    ").reset_index()\n",
    "\n",
    "all_segments = []\n",
    "all_labels = []\n",
    "all_pids = []\n",
    "skipped = 0\n",
    "\n",
    "for _, row in tqdm(grouped.iterrows(), total=len(grouped), desc='Building segments'):\n",
    "    pid = row['participant']\n",
    "    label = row['label']\n",
    "    \n",
    "    # load all waveforms\n",
    "    waveforms = []\n",
    "    for fp in row['filepaths']:\n",
    "        w = load_audio(fp, TARGET_SR)\n",
    "        if w is not None and len(w) > 0:\n",
    "            waveforms.append(w)\n",
    "    \n",
    "    if not waveforms:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    # concatenate with 50ms gaps\n",
    "    parts = []\n",
    "    gap = torch.zeros(GAP_SAMPLES)\n",
    "    for i, w in enumerate(waveforms):\n",
    "        parts.append(w)\n",
    "        if i < len(waveforms) - 1:\n",
    "            parts.append(gap)\n",
    "    combined = torch.cat(parts)\n",
    "    \n",
    "    # split into 3s segments\n",
    "    n_segments = max(1, len(combined) // SEGMENT_SAMPLES)\n",
    "    for i in range(n_segments):\n",
    "        start = i * SEGMENT_SAMPLES\n",
    "        end = start + SEGMENT_SAMPLES\n",
    "        seg = combined[start:end]\n",
    "        if len(seg) < SEGMENT_SAMPLES:\n",
    "            # center-pad\n",
    "            pad_total = SEGMENT_SAMPLES - len(seg)\n",
    "            pad_left = pad_total // 2\n",
    "            pad_right = pad_total - pad_left\n",
    "            seg = torch.nn.functional.pad(seg, (pad_left, pad_right))\n",
    "        all_segments.append(seg)\n",
    "        all_labels.append(label)\n",
    "        all_pids.append(pid)\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_pids = np.array(all_pids)\n",
    "\n",
    "pid_counts = Counter(all_pids)\n",
    "counts = list(pid_counts.values())\n",
    "\n",
    "print(f'\\nTotal segments: {len(all_segments)}')\n",
    "print(f'Skipped: {skipped}')\n",
    "print(f'Unique participants: {len(np.unique(all_pids))}')\n",
    "print(f'Segments per patient: min={min(counts)}, max={max(counts)}, mean={np.mean(counts):.1f}')\n",
    "print(f'\\nUsing Trial 1 segmentation (concatenate all + split 3s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Audio Augmentation\n",
    "\n",
    "NEW in Trial 4: augment during training to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioAugmenter:\n",
    "    \"\"\"Simple audio augmentations for cough data.\"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p  # probability of each augmentation\n",
    "    \n",
    "    def add_noise(self, waveform, snr_db=20):\n",
    "        \"\"\"Add Gaussian noise at given SNR.\"\"\"\n",
    "        if random.random() > self.p:\n",
    "            return waveform\n",
    "        snr = random.uniform(15, 30)  # 15-30 dB\n",
    "        signal_power = waveform.pow(2).mean()\n",
    "        noise_power = signal_power / (10 ** (snr / 10))\n",
    "        noise = torch.randn_like(waveform) * noise_power.sqrt()\n",
    "        return waveform + noise\n",
    "    \n",
    "    def time_shift(self, waveform, max_shift=0.2):\n",
    "        \"\"\"Shift audio left/right by up to max_shift fraction.\"\"\"\n",
    "        if random.random() > self.p:\n",
    "            return waveform\n",
    "        shift = int(len(waveform) * random.uniform(-max_shift, max_shift))\n",
    "        return torch.roll(waveform, shift)\n",
    "    \n",
    "    def amplitude_scale(self, waveform):\n",
    "        \"\"\"Scale amplitude randomly.\"\"\"\n",
    "        if random.random() > self.p:\n",
    "            return waveform\n",
    "        scale = random.uniform(0.7, 1.3)\n",
    "        return waveform * scale\n",
    "    \n",
    "    def __call__(self, waveform):\n",
    "        waveform = self.add_noise(waveform)\n",
    "        waveform = self.time_shift(waveform)\n",
    "        waveform = self.amplitude_scale(waveform)\n",
    "        return waveform\n",
    "\n",
    "augmenter = AudioAugmenter(p=0.5)\n",
    "\n",
    "# Test augmentation\n",
    "test_seg = all_segments[0].clone()\n",
    "aug_seg = augmenter(test_seg)\n",
    "print(f'Original: mean={test_seg.mean():.6f}, std={test_seg.std():.6f}')\n",
    "print(f'Augmented: mean={aug_seg.mean():.6f}, std={aug_seg.std():.6f}')\n",
    "print(f'Augmentation ready. Each aug applied with p=0.5.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Model (Unfreeze Last 2 Transformer Layers) + Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoughClassifierV2(nn.Module):\n",
    "    \"\"\"Wav2Vec2 with last 2 transformer layers unfrozen.\"\"\"\n",
    "    def __init__(self, model_name='facebook/wav2vec2-base', dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze everything first\n",
    "        for param in self.wav2vec2.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze last 2 transformer encoder layers\n",
    "        n_layers = len(self.wav2vec2.encoder.layers)\n",
    "        for layer in self.wav2vec2.encoder.layers[n_layers - 2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Unfreeze layer norm\n",
    "        if hasattr(self.wav2vec2.encoder, 'layer_norm'):\n",
    "            for param in self.wav2vec2.encoder.layer_norm.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        hidden = self.wav2vec2.config.hidden_size  # 768\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.wav2vec2(x).last_hidden_state\n",
    "        pooled = out.mean(dim=1)\n",
    "        return self.head(pooled).squeeze(-1)\n",
    "\n",
    "class CoughDatasetV2(Dataset):\n",
    "    def __init__(self, segments, labels, pids, augmenter=None):\n",
    "        self.segments = segments\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        self.pids = pids\n",
    "        self.augmenter = augmenter\n",
    "    def __len__(self): return len(self.segments)\n",
    "    def __getitem__(self, idx):\n",
    "        audio = self.segments[idx]\n",
    "        if self.augmenter is not None:\n",
    "            audio = self.augmenter(audio)\n",
    "        return {'audio': audio, 'label': self.labels[idx], 'pid': self.pids[idx]}\n",
    "\n",
    "# === Config ===\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-5  # lower LR for unfrozen layers\n",
    "EPOCHS = 10\n",
    "PATIENCE = 3  # early stopping\n",
    "GRAD_ACCUM = 8\n",
    "WARMUP_RATIO = 0.1\n",
    "N_FOLDS = 10\n",
    "\n",
    "print(f'=== Trial 4 Config ===')\n",
    "print(f'Batch: {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM})')\n",
    "print(f'Epochs: {EPOCHS} (early stop patience: {PATIENCE})')\n",
    "print(f'LR: {LR} (lower for unfrozen transformer layers)')\n",
    "print(f'Folds: {N_FOLDS}')\n",
    "print(f'Augmentation: noise + time shift + amplitude scale (p=0.5 each)')\n",
    "print(f'Classifier head: LayerNorm -> Dropout -> 768->128 GELU -> Dropout -> 128->1')\n",
    "\n",
    "# Model check\n",
    "print('\\nLoading Wav2Vec2-base (unfreeze last 2 layers)...')\n",
    "m = CoughClassifierV2()\n",
    "n_total = sum(p.numel() for p in m.parameters())\n",
    "n_train = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "n_frozen = n_total - n_train\n",
    "print(f'Total params: {n_total/1e6:.1f}M')\n",
    "print(f'Trainable: {n_train/1e6:.1f}M ({n_train/n_total*100:.1f}%)')\n",
    "print(f'Frozen: {n_frozen/1e6:.1f}M ({n_frozen/n_total*100:.1f}%)')\n",
    "\n",
    "# Count unfrozen transformer layers\n",
    "n_layers = len(m.wav2vec2.encoder.layers)\n",
    "print(f'\\nTransformer layers: {n_layers} total, last 2 unfrozen')\n",
    "for i, layer in enumerate(m.wav2vec2.encoder.layers):\n",
    "    trainable = sum(p.requires_grad for p in layer.parameters())\n",
    "    total = sum(1 for _ in layer.parameters())\n",
    "    status = 'UNFROZEN' if trainable > 0 else 'frozen'\n",
    "    if i >= n_layers - 3:  # show last 3\n",
    "        print(f'  Layer {i}: {status} ({trainable}/{total} params trainable)')\n",
    "\n",
    "dummy = torch.randn(2, SEGMENT_SAMPLES)\n",
    "with torch.no_grad(): out = m(dummy)\n",
    "print(f'\\nForward pass OK: {out.shape}')\n",
    "del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Training Function (with Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(fold_num, train_segs, train_labs, train_pids,\n",
    "                   val_segs, val_labs, val_pids):\n",
    "    train_ds = CoughDatasetV2(train_segs, train_labs, train_pids, augmenter=augmenter)\n",
    "    val_ds = CoughDatasetV2(val_segs, val_labs, val_pids, augmenter=None)  # no aug for val\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    model = CoughClassifierV2().to(DEVICE)\n",
    "    \n",
    "    # Differential LR: lower for transformer, higher for head\n",
    "    transformer_params = []\n",
    "    head_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if 'head' in name:\n",
    "                head_params.append(param)\n",
    "            else:\n",
    "                transformer_params.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': transformer_params, 'lr': LR},        # 1e-5 for transformer\n",
    "        {'params': head_params, 'lr': LR * 10},           # 1e-4 for head\n",
    "    ], weight_decay=0.01)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    total_steps = (len(train_loader) * EPOCHS) // GRAD_ACCUM\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=total_steps, eta_min=1e-7\n",
    "    )\n",
    "    \n",
    "    best_auroc = 0\n",
    "    best_patient_logits = {}\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            audio = batch['audio'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            loss = criterion(model(audio), labels) / GRAD_ACCUM\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % GRAD_ACCUM == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
    "            train_loss += loss.item() * GRAD_ACCUM\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        seg_probs, seg_labels, seg_pids = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                audio = batch['audio'].to(DEVICE)\n",
    "                probs = torch.sigmoid(model(audio)).cpu().numpy()\n",
    "                seg_probs.extend(probs)\n",
    "                seg_labels.extend(batch['label'].numpy())\n",
    "                seg_pids.extend(batch['pid'])\n",
    "        \n",
    "        # soft voting per patient\n",
    "        pt_p, pt_l = {}, {}\n",
    "        for pid, prob, lab in zip(seg_pids, seg_probs, seg_labels):\n",
    "            pt_p.setdefault(pid, []).append(prob)\n",
    "            pt_l[pid] = lab\n",
    "        \n",
    "        yt = np.array([pt_l[p] for p in pt_p])\n",
    "        yp = np.array([np.mean(v) for v in pt_p.values()])\n",
    "        auroc = roc_auc_score(yt, yp) if len(np.unique(yt)) > 1 else 0.5\n",
    "        \n",
    "        improved = ''\n",
    "        if auroc > best_auroc:\n",
    "            best_auroc = auroc\n",
    "            best_patient_logits = {pid: np.mean(v) for pid, v in pt_p.items()}\n",
    "            os.makedirs('checkpoints_t4', exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'checkpoints_t4/wav2vec2_fold{fold_num}.pt')\n",
    "            patience_counter = 0\n",
    "            improved = ' *'\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        print(f'  Epoch {epoch+1}/{EPOCHS} — loss: {train_loss:.4f}, AUROC: {auroc:.4f}{improved}')\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f'  Early stopping at epoch {epoch+1} (no improvement for {PATIENCE} epochs)')\n",
    "            break\n",
    "    \n",
    "    del model, optimizer\n",
    "    if DEVICE.type == 'mps': torch.mps.empty_cache()\n",
    "    elif DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
    "    \n",
    "    return best_auroc, best_patient_logits\n",
    "\n",
    "print('Training function ready.')\n",
    "print('Features: differential LR, cosine scheduler, early stopping, augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Run 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pids = np.unique(all_pids)\n",
    "pid_labels = np.array([all_labels[all_pids == pid][0] for pid in unique_pids])\n",
    "\n",
    "n_folds_actual = min(N_FOLDS, int(min(Counter(pid_labels).values()) * 0.8))\n",
    "skf = StratifiedKFold(n_splits=n_folds_actual, shuffle=True, random_state=42)\n",
    "\n",
    "fold_aurocs = []\n",
    "all_patient_logits = {}\n",
    "all_patient_labels = {}\n",
    "\n",
    "print(f'=== TRIAL 4: Push Wav2Vec2 to the Limit ===')\n",
    "print(f'Changes: unfreeze 2 layers, augmentation, 10 epochs, early stop, differential LR')\n",
    "print(f'Segmentation: Trial 1 (concatenate all + split 3s)')\n",
    "print(f'Segments: {len(all_segments)}')\n",
    "print(f'Patients: {len(unique_pids)}')\n",
    "print(f'Folds: {n_folds_actual}, Device: {DEVICE}')\n",
    "print(f'\\n--- Benchmarks ---')\n",
    "print(f'DREAM Challenge winner: 0.743')\n",
    "print(f'Trial 1 (baseline): 0.733')\n",
    "print(f'Zambia (TB+ vs OR): 0.801')\n",
    "print(f'\\nStarting training...\\n')\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(unique_pids, pid_labels)):\n",
    "    print(f'=== Fold {fold+1}/{n_folds_actual} ===')\n",
    "    \n",
    "    train_pids_set = set(unique_pids[train_idx])\n",
    "    val_pids_set = set(unique_pids[val_idx])\n",
    "    \n",
    "    tr_s, tr_l, tr_p = [], [], []\n",
    "    va_s, va_l, va_p = [], [], []\n",
    "    \n",
    "    for seg, lab, pid in zip(all_segments, all_labels, all_pids):\n",
    "        if pid in train_pids_set:\n",
    "            tr_s.append(seg); tr_l.append(lab); tr_p.append(pid)\n",
    "        else:\n",
    "            va_s.append(seg); va_l.append(lab); va_p.append(pid)\n",
    "    \n",
    "    print(f'  Train: {len(tr_s)} segments ({len(train_pids_set)} patients)')\n",
    "    print(f'  Val:   {len(va_s)} segments ({len(val_pids_set)} patients)')\n",
    "    \n",
    "    auroc, patient_logits = train_one_fold(\n",
    "        fold+1, tr_s, tr_l, tr_p, va_s, va_l, va_p\n",
    "    )\n",
    "    \n",
    "    fold_aurocs.append(auroc)\n",
    "    all_patient_logits.update(patient_logits)\n",
    "    for pid in val_pids_set:\n",
    "        all_patient_labels[pid] = pid_labels[unique_pids == pid][0]\n",
    "    \n",
    "    print(f'  \\u2705 Fold {fold+1} best AUROC: {auroc:.4f}\\n')\n",
    "\n",
    "print('=' * 50)\n",
    "print(f'TRIAL 4 RESULT')\n",
    "print(f'Mean AUROC: {np.mean(fold_aurocs):.4f} +/- {np.std(fold_aurocs):.4f}')\n",
    "print(f'Per-fold: {[f\"{a:.3f}\" for a in fold_aurocs]}')\n",
    "print(f'\\n--- Full Comparison ---')\n",
    "print(f'DREAM winner: 0.743')\n",
    "print(f'Trial 1: 0.7330 +/- 0.0565')\n",
    "print(f'Trial 2: 0.6926 +/- 0.0438')\n",
    "print(f'Trial 3: 0.7247 +/- 0.0724')\n",
    "print(f'Trial 4: {np.mean(fold_aurocs):.4f} +/- {np.std(fold_aurocs):.4f}')\n",
    "beat_dream = np.mean(fold_aurocs) > 0.743\n",
    "print(f'\\n{\"\\u2705 BEAT DREAM CHALLENGE WINNER!\" if beat_dream else \"\\u26a0\\ufe0f Did not beat DREAM winner. Switch to HeAR (Trial 5).\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Results — ROC + Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_order = sorted(all_patient_logits.keys())\n",
    "y_true = np.array([all_patient_labels[p] for p in pids_order])\n",
    "y_prob = np.array([all_patient_logits[p] for p in pids_order])\n",
    "\n",
    "fpr_t4, tpr_t4, _ = roc_curve(y_true, y_prob)\n",
    "auroc_t4 = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.plot(fpr_t4, tpr_t4, 'r-', lw=2.5, label=f'Trial 4 ({auroc_t4:.3f})')\n",
    "plt.plot([0,1],[0,1],'k--', alpha=0.2)\n",
    "plt.axhspan(0.90, 1.0, xmin=0, xmax=0.30, alpha=0.08, color='green', label='WHO TPP zone')\n",
    "plt.axhline(0.90, color='r', ls=':', alpha=0.3)\n",
    "plt.axvline(0.30, color='g', ls=':', alpha=0.3)\n",
    "plt.xlabel('FPR (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('TPR (Sensitivity)', fontsize=12)\n",
    "plt.title('RespiraHub Trial 4 — Wav2Vec2 Optimized', fontsize=14)\n",
    "plt.legend(fontsize=11); plt.grid(alpha=0.15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_trial4.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Thresh\":>7} {\"Sens\":>7} {\"Spec\":>7} {\"PPV\":>7} {\"NPV\":>7}')\n",
    "print('-' * 40)\n",
    "\n",
    "best_t, best_j = 0.5, -1\n",
    "who_met = False\n",
    "\n",
    "for t in np.arange(0.15, 0.85, 0.05):\n",
    "    pred = (y_prob >= t).astype(int)\n",
    "    tp = np.sum((pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((pred == 0) & (y_true == 1))\n",
    "    sens = tp/(tp+fn) if (tp+fn) else 0\n",
    "    spec = tn/(tn+fp) if (tn+fp) else 0\n",
    "    ppv = tp/(tp+fp) if (tp+fp) else 0\n",
    "    npv = tn/(tn+fn) if (tn+fn) else 0\n",
    "    \n",
    "    flag = ' \\u2705 WHO' if (sens >= 0.90 and spec >= 0.70) else ''\n",
    "    if sens >= 0.90 and spec >= 0.70: who_met = True\n",
    "    \n",
    "    j = sens + spec - 1\n",
    "    if j > best_j: best_j, best_t = j, t\n",
    "    \n",
    "    print(f'{t:>7.2f} {sens:>7.3f} {spec:>7.3f} {ppv:>7.3f} {npv:>7.3f}{flag}')\n",
    "\n",
    "print(f'\\nBest Youden threshold: {best_t:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Adversarial Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold = np.argmax(fold_aurocs) + 1\n",
    "model = CoughClassifierV2().to(DEVICE)\n",
    "model.load_state_dict(torch.load(f'checkpoints_t4/wav2vec2_fold{best_fold}.pt', map_location=DEVICE))\n",
    "model.eval()\n",
    "print(f'Loaded fold {best_fold} (AUROC={fold_aurocs[best_fold-1]:.4f})')\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(20, SEGMENT_SAMPLES).to(DEVICE)\n",
    "    p_noise = torch.sigmoid(model(noise)).cpu().numpy()\n",
    "    print(f'White noise  -> P(TB+) = {p_noise.mean():.4f} +/- {p_noise.std():.4f}')\n",
    "    \n",
    "    silence = torch.zeros(20, SEGMENT_SAMPLES).to(DEVICE)\n",
    "    p_silence = torch.sigmoid(model(silence)).cpu().numpy()\n",
    "    print(f'Silence      -> P(TB+) = {p_silence.mean():.4f} +/- {p_silence.std():.4f}')\n",
    "    \n",
    "    t_ax = torch.linspace(0, SEGMENT_SEC, SEGMENT_SAMPLES)\n",
    "    hum = (torch.sin(2*3.14159*50*t_ax)*0.01).unsqueeze(0).repeat(20,1).to(DEVICE)\n",
    "    p_hum = torch.sigmoid(model(hum)).cpu().numpy()\n",
    "    print(f'50Hz hum     -> P(TB+) = {p_hum.mean():.4f} +/- {p_hum.std():.4f}')\n",
    "\n",
    "print('\\nAll should be ~0.50. If >> 0.60, model overfitting to background.')\n",
    "del model\n",
    "if DEVICE.type == 'mps': torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Save Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'participant': pids_order,\n",
    "    'true_label': y_true,\n",
    "    'predicted_prob': y_prob,\n",
    "})\n",
    "results_df.to_csv('patient_predictions_trial4.csv', index=False)\n",
    "\n",
    "summary = {\n",
    "    'trial': 4,\n",
    "    'changes': [\n",
    "        'unfreeze last 2 transformer layers',\n",
    "        'audio augmentation (noise, time shift, amplitude)',\n",
    "        '10 epochs with early stopping (patience 3)',\n",
    "        'differential LR (1e-5 transformer, 1e-4 head)',\n",
    "        'cosine annealing scheduler',\n",
    "        'deeper classifier head (768->128->1 with GELU)',\n",
    "        'Trial 1 segmentation (concatenate all + split 3s)',\n",
    "    ],\n",
    "    'model': 'Wav2Vec2-base (last 2 layers unfrozen)',\n",
    "    'dataset': 'CODA TB solicited',\n",
    "    'n_participants': len(pids_order),\n",
    "    'n_segments': len(all_segments),\n",
    "    'segments_per_patient_mean': round(float(np.mean(counts)), 1),\n",
    "    'segment_sec': SEGMENT_SEC,\n",
    "    'n_folds': n_folds_actual,\n",
    "    'auroc_mean': round(float(np.mean(fold_aurocs)), 4),\n",
    "    'auroc_std': round(float(np.std(fold_aurocs)), 4),\n",
    "    'auroc_per_fold': [round(float(a), 4) for a in fold_aurocs],\n",
    "    'best_threshold': round(float(best_t), 2),\n",
    "    'dream_winner': 0.743,\n",
    "    'trial1_auroc': 0.7330,\n",
    "    'trial2_auroc': 0.6926,\n",
    "    'trial3_auroc': 0.7247,\n",
    "    'zambia_auroc': 0.852,\n",
    "    'device': str(DEVICE),\n",
    "}\n",
    "with open('training_summary_trial4.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('Saved:')\n",
    "print('  patient_predictions_trial4.csv')\n",
    "print('  training_summary_trial4.json')\n",
    "print('  checkpoints_t4/wav2vec2_fold*.pt')\n",
    "print('  roc_trial4.png')\n",
    "print()\n",
    "print('=' * 50)\n",
    "print('TRIAL 4 COMPLETE')\n",
    "print('=' * 50)\n",
    "print(f'DREAM winner: 0.743')\n",
    "print(f'Trial 1: 0.7330 +/- 0.0565')\n",
    "print(f'Trial 2: 0.6926 +/- 0.0438')\n",
    "print(f'Trial 3: 0.7247 +/- 0.0724')\n",
    "print(f'Trial 4: {np.mean(fold_aurocs):.4f} +/- {np.std(fold_aurocs):.4f}')\n",
    "print(f'\\nNext: Trial 5 (HeAR backbone) regardless of result.')\n",
    "print(f'Trial 4 is the final Wav2Vec2 experiment.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
