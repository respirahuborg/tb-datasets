{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RespiraHub — Trial 6: Dual-Backbone Ensemble (Wav2Vec2 + HeAR)\n",
    "\n",
    "**Insight:** Wav2Vec2 (speech) dan HeAR (health) dapet AUROC mirip (~0.72) tapi kemungkinan capture features yang beda. Combine = complementary signal.\n",
    "\n",
    "**Approach:**\n",
    "1. Extract Wav2Vec2 embeddings (frozen, 768-dim) dari 2s segments\n",
    "2. Extract HeAR embeddings (frozen, 1024-dim) dari 2s segments — udah ada\n",
    "3. Concatenate → 1792-dim per segment\n",
    "4. Train MLP classifier di atas combined representation\n",
    "\n",
    "**Why this might work:**\n",
    "- Wav2Vec2: good at temporal speech patterns, learned from 960h Librispeech\n",
    "- HeAR: good at health acoustic events, learned from 313M health clips\n",
    "- Different pre-training = different feature spaces = complementary\n",
    "\n",
    "**Target:** 0.75+ (beat DREAM Challenge winner 0.743)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Model, AutoModel\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HeAR preprocessing\n",
    "audio_utils = importlib.import_module('hear.python.data_processing.audio_utils')\n",
    "preprocess_audio = audio_utils.preprocess_audio\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'PyTorch {torch.__version__}, Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load Metadata + Build 2s Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = '/Users/aida/code/development/tb-datasets/data/solicited/'\n",
    "CLINICAL_PATH = '/Users/aida/code/development/tb-datasets/data/metadata/CODA_TB_Clinical_Meta_Info.csv'\n",
    "ADDITIONAL_PATH = '/Users/aida/code/development/tb-datasets/data/metadata/CODA_TB_additional_variables_train.csv'\n",
    "SOLICITED_PATH = '/Users/aida/code/development/tb-datasets/data/metadata/CODA_TB_Solicited_Meta_Info.csv'\n",
    "\n",
    "clinical = pd.read_csv(CLINICAL_PATH)\n",
    "additional = pd.read_csv(ADDITIONAL_PATH)\n",
    "solicited = pd.read_csv(SOLICITED_PATH)\n",
    "\n",
    "meta = clinical.merge(additional, on='participant', how='left')\n",
    "meta['label'] = meta['tb_status'].astype(int)\n",
    "\n",
    "df = solicited.merge(meta, on='participant', how='left')\n",
    "df['filepath'] = df['filename'].apply(lambda f: os.path.join(AUDIO_DIR, f))\n",
    "df['file_exists'] = df['filepath'].apply(os.path.exists)\n",
    "df = df[df['file_exists']].reset_index(drop=True)\n",
    "\n",
    "print(f'Participants: {meta[\"label\"].count()}, TB+: {meta[\"label\"].sum()}')\n",
    "print(f'Cough files: {len(df)} from {df[\"participant\"].nunique()} participants')\n",
    "\n",
    "# === Segmentation: concatenate all + split 2s (same as Trial 5) ===\n",
    "TARGET_SR = 16000\n",
    "SEGMENT_SEC = 2.0\n",
    "SEGMENT_SAMPLES = int(TARGET_SR * SEGMENT_SEC)  # 32000\n",
    "GAP_SAMPLES = int(TARGET_SR * 0.05)\n",
    "\n",
    "def load_audio(filepath, target_sr=16000):\n",
    "    try:\n",
    "        w, sr = torchaudio.load(filepath)\n",
    "        if w.shape[0] > 1:\n",
    "            w = w.mean(dim=0, keepdim=True)\n",
    "        if sr != target_sr:\n",
    "            w = torchaudio.transforms.Resample(sr, target_sr)(w)\n",
    "        return w.squeeze(0)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "grouped = df.groupby('participant').agg(\n",
    "    label=('label', 'first'),\n",
    "    filepaths=('filepath', list),\n",
    ").reset_index()\n",
    "\n",
    "all_segments = []\n",
    "all_labels = []\n",
    "all_pids = []\n",
    "\n",
    "for _, row in tqdm(grouped.iterrows(), total=len(grouped), desc='Building 2s segments'):\n",
    "    pid = row['participant']\n",
    "    label = row['label']\n",
    "    waveforms = []\n",
    "    for fp in row['filepaths']:\n",
    "        w = load_audio(fp, TARGET_SR)\n",
    "        if w is not None and len(w) > 0:\n",
    "            waveforms.append(w)\n",
    "    if not waveforms:\n",
    "        continue\n",
    "    parts = []\n",
    "    gap = torch.zeros(GAP_SAMPLES)\n",
    "    for i, w in enumerate(waveforms):\n",
    "        parts.append(w)\n",
    "        if i < len(waveforms) - 1:\n",
    "            parts.append(gap)\n",
    "    combined = torch.cat(parts)\n",
    "    n_segments = max(1, len(combined) // SEGMENT_SAMPLES)\n",
    "    for i in range(n_segments):\n",
    "        start = i * SEGMENT_SAMPLES\n",
    "        end = start + SEGMENT_SAMPLES\n",
    "        seg = combined[start:end]\n",
    "        if len(seg) < SEGMENT_SAMPLES:\n",
    "            pad_total = SEGMENT_SAMPLES - len(seg)\n",
    "            pad_left = pad_total // 2\n",
    "            pad_right = pad_total - pad_left\n",
    "            seg = torch.nn.functional.pad(seg, (pad_left, pad_right))\n",
    "        all_segments.append(seg)\n",
    "        all_labels.append(label)\n",
    "        all_pids.append(pid)\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_pids = np.array(all_pids)\n",
    "counts = list(Counter(all_pids).values())\n",
    "\n",
    "print(f'\\nSegments: {len(all_segments)} @ 2s')\n",
    "print(f'Patients: {len(np.unique(all_pids))}')\n",
    "print(f'Seg/patient: mean={np.mean(counts):.1f}, min={min(counts)}, max={max(counts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Load Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Wav2Vec2 ===\n",
    "print('Loading Wav2Vec2-base...')\n",
    "w2v_model = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base')\n",
    "w2v_model = w2v_model.to(DEVICE)\n",
    "w2v_model.eval()\n",
    "for param in w2v_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(f'  Wav2Vec2: {sum(p.numel() for p in w2v_model.parameters())/1e6:.1f}M params (frozen)')\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    test = torch.randn(1, SEGMENT_SAMPLES).to(DEVICE)\n",
    "    w2v_out = w2v_model(test).last_hidden_state.mean(dim=1)\n",
    "    W2V_DIM = w2v_out.shape[-1]\n",
    "print(f'  Wav2Vec2 embedding dim: {W2V_DIM}')\n",
    "\n",
    "# === HeAR ===\n",
    "print('\\nLoading HeAR...')\n",
    "hear_model = AutoModel.from_pretrained('google/hear-pytorch', trust_remote_code=True)\n",
    "hear_model = hear_model.to(DEVICE)\n",
    "hear_model.eval()\n",
    "for param in hear_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(f'  HeAR: {sum(p.numel() for p in hear_model.parameters())/1e6:.1f}M params (frozen)')\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    test_spec = preprocess_audio(torch.randn(1, SEGMENT_SAMPLES))\n",
    "    hear_out = hear_model.forward(test_spec.to(DEVICE), return_dict=True, output_hidden_states=True)\n",
    "    hear_emb = hear_out.last_hidden_state.mean(dim=1)\n",
    "    HEAR_DIM = hear_emb.shape[-1]\n",
    "print(f'  HeAR embedding dim: {HEAR_DIM}')\n",
    "\n",
    "COMBINED_DIM = W2V_DIM + HEAR_DIM\n",
    "print(f'\\nCombined embedding: {W2V_DIM} + {HEAR_DIM} = {COMBINED_DIM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Extract Dual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_BATCH = 8  # smaller batch since 2 models\n",
    "w2v_embeddings = []\n",
    "hear_embeddings = []\n",
    "\n",
    "print(f'Extracting dual embeddings for {len(all_segments)} segments...')\n",
    "print(f'Wav2Vec2 ({W2V_DIM}-dim) + HeAR ({HEAR_DIM}-dim)')\n",
    "\n",
    "for i in tqdm(range(0, len(all_segments), EMBED_BATCH), desc='Extracting'):\n",
    "    batch_segs = all_segments[i:i + EMBED_BATCH]\n",
    "    batch_audio = torch.stack(batch_segs)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Wav2Vec2: raw audio → mean pool\n",
    "        w2v_out = w2v_model(batch_audio.to(DEVICE)).last_hidden_state\n",
    "        w2v_emb = w2v_out.mean(dim=1).cpu()\n",
    "        \n",
    "        # HeAR: raw audio → spectrogram → mean pool\n",
    "        batch_spec = preprocess_audio(batch_audio)\n",
    "        hear_out = hear_model.forward(\n",
    "            batch_spec.to(DEVICE),\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hear_emb = hear_out.last_hidden_state.mean(dim=1).cpu()\n",
    "    \n",
    "    w2v_embeddings.append(w2v_emb)\n",
    "    hear_embeddings.append(hear_emb)\n",
    "\n",
    "w2v_embeddings = torch.cat(w2v_embeddings, dim=0)\n",
    "hear_embeddings = torch.cat(hear_embeddings, dim=0)\n",
    "\n",
    "# Concatenate\n",
    "all_embeddings = torch.cat([w2v_embeddings, hear_embeddings], dim=1)\n",
    "\n",
    "print(f'\\nWav2Vec2 embeddings: {w2v_embeddings.shape}')\n",
    "print(f'HeAR embeddings: {hear_embeddings.shape}')\n",
    "print(f'Combined embeddings: {all_embeddings.shape}')\n",
    "print(f'Memory: {all_embeddings.nbytes / 1024 / 1024:.1f} MB')\n",
    "\n",
    "# Save\n",
    "torch.save({\n",
    "    'w2v_embeddings': w2v_embeddings,\n",
    "    'hear_embeddings': hear_embeddings,\n",
    "    'combined_embeddings': all_embeddings,\n",
    "    'labels': all_labels,\n",
    "    'pids': all_pids,\n",
    "}, 'dual_embeddings.pt')\n",
    "print('Saved dual_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Classifier + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = all_embeddings.shape[1]  # auto-detect\n",
    "\n",
    "class DualBackboneClassifier(nn.Module):\n",
    "    \"\"\"Classifier on concatenated Wav2Vec2 + HeAR embeddings.\"\"\"\n",
    "    def __init__(self, embed_dim=EMBED_DIM, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels, pids):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        self.pids = pids\n",
    "    def __len__(self): return len(self.embeddings)\n",
    "    def __getitem__(self, idx):\n",
    "        return {'emb': self.embeddings[idx], 'label': self.labels[idx], 'pid': self.pids[idx]}\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LR = 5e-4\n",
    "EPOCHS = 50\n",
    "PATIENCE = 7\n",
    "N_FOLDS = 10\n",
    "\n",
    "m = DualBackboneClassifier()\n",
    "n_params = sum(p.numel() for p in m.parameters())\n",
    "print(f'=== Trial 6 Config ===')\n",
    "print(f'Input dim: {EMBED_DIM} (Wav2Vec2 {W2V_DIM} + HeAR {HEAR_DIM})')\n",
    "print(f'Classifier: {n_params:,} params ({n_params/1e3:.1f}K)')\n",
    "print(f'Architecture: {EMBED_DIM}→256→64→1 (3-layer MLP)')\n",
    "print(f'Batch: {BATCH_SIZE}, LR: {LR}, Epochs: {EPOCHS}, Patience: {PATIENCE}')\n",
    "del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(fold_num, tr_emb, tr_lab, tr_pid, va_emb, va_lab, va_pid):\n",
    "    train_ds = EmbeddingDataset(tr_emb, tr_lab, tr_pid)\n",
    "    val_ds = EmbeddingDataset(va_emb, va_lab, va_pid)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = DualBackboneClassifier().to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "    \n",
    "    best_auroc = 0\n",
    "    best_patient_logits = {}\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            emb = batch['emb'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(emb), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        seg_probs, seg_labels, seg_pids = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                emb = batch['emb'].to(DEVICE)\n",
    "                probs = torch.sigmoid(model(emb)).cpu().numpy()\n",
    "                seg_probs.extend(probs)\n",
    "                seg_labels.extend(batch['label'].numpy())\n",
    "                seg_pids.extend(batch['pid'])\n",
    "        \n",
    "        pt_p, pt_l = {}, {}\n",
    "        for pid, prob, lab in zip(seg_pids, seg_probs, seg_labels):\n",
    "            pt_p.setdefault(pid, []).append(prob)\n",
    "            pt_l[pid] = lab\n",
    "        \n",
    "        yt = np.array([pt_l[p] for p in pt_p])\n",
    "        yp = np.array([np.mean(v) for v in pt_p.values()])\n",
    "        auroc = roc_auc_score(yt, yp) if len(np.unique(yt)) > 1 else 0.5\n",
    "        \n",
    "        improved = ''\n",
    "        if auroc > best_auroc:\n",
    "            best_auroc = auroc\n",
    "            best_patient_logits = {pid: np.mean(v) for pid, v in pt_p.items()}\n",
    "            os.makedirs('checkpoints_t6', exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'checkpoints_t6/dual_fold{fold_num}.pt')\n",
    "            patience_counter = 0\n",
    "            improved = ' *'\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or improved or patience_counter >= PATIENCE:\n",
    "            print(f'  Epoch {epoch+1}/{EPOCHS} — loss: {train_loss:.4f}, AUROC: {auroc:.4f}{improved}')\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f'  Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    del model, optimizer\n",
    "    return best_auroc, best_patient_logits\n",
    "\n",
    "print('Training function ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Run 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pids = np.unique(all_pids)\n",
    "pid_labels = np.array([all_labels[all_pids == pid][0] for pid in unique_pids])\n",
    "\n",
    "n_folds_actual = min(N_FOLDS, int(min(Counter(pid_labels).values()) * 0.8))\n",
    "skf = StratifiedKFold(n_splits=n_folds_actual, shuffle=True, random_state=42)\n",
    "\n",
    "fold_aurocs = []\n",
    "all_patient_logits = {}\n",
    "all_patient_labels = {}\n",
    "\n",
    "print(f'=== TRIAL 6: Dual-Backbone Ensemble ===')\n",
    "print(f'Backbones: Wav2Vec2 ({W2V_DIM}d) + HeAR ({HEAR_DIM}d) = {COMBINED_DIM}d')\n",
    "print(f'Classifier: 3-layer MLP')\n",
    "print(f'Segments: {len(all_segments)} @ 2s')\n",
    "print(f'Patients: {len(unique_pids)}')\n",
    "print(f'Folds: {n_folds_actual}, Device: {DEVICE}')\n",
    "print(f'\\n--- Benchmarks ---')\n",
    "print(f'DREAM winner:  0.743')\n",
    "print(f'Trial 1 (W2V): 0.718 (re-run baseline)')\n",
    "print(f'Trial 5 (HeAR): 0.719')\n",
    "print(f'\\nStarting training...\\n')\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(unique_pids, pid_labels)):\n",
    "    print(f'=== Fold {fold+1}/{n_folds_actual} ===')\n",
    "    \n",
    "    train_pids_set = set(unique_pids[train_idx])\n",
    "    val_pids_set = set(unique_pids[val_idx])\n",
    "    \n",
    "    tr_mask = np.array([pid in train_pids_set for pid in all_pids])\n",
    "    va_mask = ~tr_mask\n",
    "    \n",
    "    tr_emb = all_embeddings[tr_mask]\n",
    "    tr_lab = all_labels[tr_mask]\n",
    "    tr_pid = all_pids[tr_mask]\n",
    "    va_emb = all_embeddings[va_mask]\n",
    "    va_lab = all_labels[va_mask]\n",
    "    va_pid = all_pids[va_mask]\n",
    "    \n",
    "    print(f'  Train: {len(tr_emb)} embeddings ({len(train_pids_set)} patients)')\n",
    "    print(f'  Val:   {len(va_emb)} embeddings ({len(val_pids_set)} patients)')\n",
    "    \n",
    "    auroc, patient_logits = train_one_fold(\n",
    "        fold+1, tr_emb, tr_lab, tr_pid, va_emb, va_lab, va_pid\n",
    "    )\n",
    "    \n",
    "    fold_aurocs.append(auroc)\n",
    "    all_patient_logits.update(patient_logits)\n",
    "    for pid in val_pids_set:\n",
    "        all_patient_labels[pid] = pid_labels[unique_pids == pid][0]\n",
    "    \n",
    "    print(f'  \\u2705 Fold {fold+1} best AUROC: {auroc:.4f}\\n')\n",
    "\n",
    "print('=' * 60)\n",
    "print(f'TRIAL 6 RESULT (Dual-Backbone Ensemble)')\n",
    "print(f'Mean AUROC: {np.mean(fold_aurocs):.4f} +/- {np.std(fold_aurocs):.4f}')\n",
    "print(f'Per-fold: {[f\"{a:.3f}\" for a in fold_aurocs]}')\n",
    "print(f'\\n--- Full Comparison ---')\n",
    "print(f'DREAM winner:       0.743')\n",
    "print(f'Trial 1 (W2V 3s):   0.718')\n",
    "print(f'Trial 5 (HeAR 2s):  0.719')\n",
    "print(f'Trial 6 (W2V+HeAR): {np.mean(fold_aurocs):.4f} +/- {np.std(fold_aurocs):.4f}')\n",
    "\n",
    "beat_single = np.mean(fold_aurocs) > 0.719\n",
    "beat_dream = np.mean(fold_aurocs) > 0.743\n",
    "msg = '\\u2705 BEAT DREAM CHALLENGE WINNER!' if beat_dream else ('\\u2705 Beat both single backbones!' if beat_single else '\\u26a0\\ufe0f Did not beat single backbones.')\n",
    "print(f'\\n{msg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: ROC + Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_order = sorted(all_patient_logits.keys())\n",
    "y_true = np.array([all_patient_labels[p] for p in pids_order])\n",
    "y_prob = np.array([all_patient_logits[p] for p in pids_order])\n",
    "\n",
    "fpr_t6, tpr_t6, _ = roc_curve(y_true, y_prob)\n",
    "auroc_t6 = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.plot(fpr_t6, tpr_t6, 'r-', lw=2.5, label=f'Trial 6 Ensemble ({auroc_t6:.3f})')\n",
    "plt.plot([0,1],[0,1],'k--', alpha=0.2)\n",
    "plt.axhspan(0.90, 1.0, xmin=0, xmax=0.30, alpha=0.08, color='green', label='WHO TPP zone')\n",
    "plt.axhline(0.90, color='r', ls=':', alpha=0.3)\n",
    "plt.axvline(0.30, color='g', ls=':', alpha=0.3)\n",
    "plt.xlabel('FPR (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('TPR (Sensitivity)', fontsize=12)\n",
    "plt.title('RespiraHub Trial 6 \\u2014 Dual-Backbone Ensemble', fontsize=14)\n",
    "plt.legend(fontsize=11); plt.grid(alpha=0.15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_trial6.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Thresh\":>7} {\"Sens\":>7} {\"Spec\":>7} {\"PPV\":>7} {\"NPV\":>7}')\n",
    "print('-' * 40)\n",
    "\n",
    "best_t, best_j = 0.5, -1\n",
    "\n",
    "for t in np.arange(0.15, 0.85, 0.05):\n",
    "    pred = (y_prob >= t).astype(int)\n",
    "    tp = np.sum((pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((pred == 0) & (y_true == 1))\n",
    "    sens = tp/(tp+fn) if (tp+fn) else 0\n",
    "    spec = tn/(tn+fp) if (tn+fp) else 0\n",
    "    ppv = tp/(tp+fp) if (tp+fp) else 0\n",
    "    npv = tn/(tn+fn) if (tn+fn) else 0\n",
    "    flag = ' \\u2705 WHO' if (sens >= 0.90 and spec >= 0.70) else ''\n",
    "    j = sens + spec - 1\n",
    "    if j > best_j: best_j, best_t = j, t\n",
    "    print(f'{t:>7.2f} {sens:>7.3f} {spec:>7.3f} {ppv:>7.3f} {npv:>7.3f}{flag}')\n",
    "\n",
    "print(f'\\nBest Youden threshold: {best_t:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'participant': pids_order,\n",
    "    'true_label': y_true,\n",
    "    'predicted_prob': y_prob,\n",
    "})\n",
    "results_df.to_csv('patient_predictions_trial6.csv', index=False)\n",
    "\n",
    "summary = {\n",
    "    'trial': 6,\n",
    "    'approach': 'Dual-backbone ensemble: Wav2Vec2 + HeAR concatenated embeddings',\n",
    "    'backbones': ['facebook/wav2vec2-base', 'google/hear-pytorch'],\n",
    "    'embedding_dims': {'wav2vec2': W2V_DIM, 'hear': HEAR_DIM, 'combined': COMBINED_DIM},\n",
    "    'n_participants': len(pids_order),\n",
    "    'n_segments': len(all_segments),\n",
    "    'segment_sec': SEGMENT_SEC,\n",
    "    'n_folds': n_folds_actual,\n",
    "    'auroc_mean': round(float(np.mean(fold_aurocs)), 4),\n",
    "    'auroc_std': round(float(np.std(fold_aurocs)), 4),\n",
    "    'auroc_per_fold': [round(float(a), 4) for a in fold_aurocs],\n",
    "    'best_threshold': round(float(best_t), 2),\n",
    "    'device': str(DEVICE),\n",
    "}\n",
    "with open('training_summary_trial6.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('Saved:')\n",
    "print('  patient_predictions_trial6.csv')\n",
    "print('  training_summary_trial6.json')\n",
    "print('  dual_embeddings.pt')\n",
    "print('  checkpoints_t6/dual_fold*.pt')\n",
    "print('  roc_trial6.png')\n",
    "print()\n",
    "print('=' * 60)\n",
    "print('TRIAL 6 COMPLETE')\n",
    "print('=' * 60)\n",
    "print(f'Trial 1 (W2V only):   0.718')\n",
    "print(f'Trial 5 (HeAR only):  0.719')\n",
    "print(f'Trial 6 (Ensemble):   {np.mean(fold_aurocs):.4f} +/- {np.std(fold_aurocs):.4f}')\n",
    "print(f'DREAM winner:         0.743')\n",
    "print(f'\\nDelta vs best single: {np.mean(fold_aurocs) - 0.719:+.4f}')\n",
    "print(f'\\nThis is the key result for the benchmark paper.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
