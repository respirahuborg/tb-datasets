{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RespiraHub — Trial 7: Multimodal (Audio + Clinical Anamnesis)\n",
    "\n",
    "**Insight:** 6 trial audio-only semua converge di 0.69-0.73. Zambia study: audio+clinical = 0.921 vs audio-only = 0.852 (+7%). Time to add clinical features.\n",
    "\n",
    "**Approach:**\n",
    "1. Reuse dual embeddings dari Trial 6 (Wav2Vec2 768d + HeAR 1024d = 1792d)\n",
    "2. Encode clinical features dari CODA TB metadata (~15-20 features)\n",
    "3. Concatenate: 1792d audio + Nd clinical = combined\n",
    "4. Train MLP classifier\n",
    "\n",
    "**Available clinical features (almost zero missing!):**\n",
    "- Demographics: sex, age, height, weight (→ BMI)\n",
    "- Symptoms: reported_cough_dur, hemoptysis, weight_loss, fever, night_sweats\n",
    "- History: tb_prior, tb_prior_Pul, tb_prior_Extrapul, smoke_lweek\n",
    "- Vitals: heart_rate, temperature\n",
    "- HIV: HIVstatus (3 missing)\n",
    "- Context: Country\n",
    "\n",
    "**Target:** 0.78+ (beat DREAM Challenge winner 0.743 decisively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'PyTorch {torch.__version__}, Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load Audio Embeddings from Trial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-extracted dual embeddings from Trial 6\n",
    "data = torch.load('dual_embeddings.pt', weights_only=False)\n",
    "audio_embeddings = data['combined_embeddings']  # (2216, 1792)\n",
    "all_labels = data['labels']  # (2216,)\n",
    "all_pids = data['pids']  # (2216,)\n",
    "\n",
    "AUDIO_DIM = audio_embeddings.shape[1]\n",
    "print(f'Audio embeddings: {audio_embeddings.shape}')\n",
    "print(f'Segments: {len(audio_embeddings)}')\n",
    "print(f'Patients: {len(np.unique(all_pids))}')\n",
    "print(f'Audio dim: {AUDIO_DIM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Load & Engineer Clinical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLINICAL_PATH = '/Users/aida/code/development/tb-datasets/data/metadata/CODA_TB_Clinical_Meta_Info.csv'\n",
    "ADDITIONAL_PATH = '/Users/aida/code/development/tb-datasets/data/metadata/CODA_TB_additional_variables_train.csv'\n",
    "\n",
    "clinical = pd.read_csv(CLINICAL_PATH)\n",
    "additional = pd.read_csv(ADDITIONAL_PATH)\n",
    "meta = clinical.merge(additional, on='participant', how='left')\n",
    "\n",
    "print(f'Clinical data: {meta.shape[0]} participants, {meta.shape[1]} columns')\n",
    "print(f'TB+: {meta[\"tb_status\"].sum()}, TB-: {(meta[\"tb_status\"]==0).sum()}')\n",
    "\n",
    "# === Feature Engineering ===\n",
    "\n",
    "# Binary encode Yes/No columns\n",
    "binary_cols = ['hemoptysis', 'weight_loss', 'fever', 'night_sweats', \n",
    "               'smoke_lweek', 'tb_prior', 'tb_prior_Pul', 'tb_prior_Extrapul']\n",
    "for col in binary_cols:\n",
    "    meta[f'{col}_bin'] = (meta[col] == 'Yes').astype(float)\n",
    "\n",
    "# Sex: Male=1, Female=0\n",
    "meta['sex_bin'] = (meta['sex'] == 'Male').astype(float)\n",
    "\n",
    "# HIV: Positive=1, Negative=0, missing=0.5 (neutral imputation)\n",
    "meta['hiv_bin'] = meta['HIVstatus'].map({'Positive': 1.0, 'Negative': 0.0}).fillna(0.5)\n",
    "\n",
    "# BMI\n",
    "meta['bmi'] = meta['weight'] / ((meta['height']/100) ** 2)\n",
    "meta['bmi'] = meta['bmi'].fillna(meta['bmi'].median())\n",
    "\n",
    "# Cough duration buckets (clinical relevance: >2 weeks is TB red flag)\n",
    "meta['cough_gt14d'] = (meta['reported_cough_dur'] > 14).astype(float)\n",
    "meta['cough_gt30d'] = (meta['reported_cough_dur'] > 30).astype(float)\n",
    "\n",
    "# Country one-hot (important: different TB strains, recording conditions)\n",
    "country_dummies = pd.get_dummies(meta['Country'], prefix='country').astype(float)\n",
    "meta = pd.concat([meta, country_dummies], axis=1)\n",
    "country_cols = [c for c in meta.columns if c.startswith('country_')]\n",
    "\n",
    "# Symptom count (composite score)\n",
    "symptom_cols_bin = ['hemoptysis_bin', 'weight_loss_bin', 'fever_bin', 'night_sweats_bin']\n",
    "meta['symptom_count'] = meta[symptom_cols_bin].sum(axis=1)\n",
    "\n",
    "# === Define final feature set ===\n",
    "CONTINUOUS_FEATURES = ['age', 'bmi', 'heart_rate', 'temperature', \n",
    "                       'reported_cough_dur', 'symptom_count']\n",
    "\n",
    "BINARY_FEATURES = ['sex_bin', 'hiv_bin',\n",
    "                   'hemoptysis_bin', 'weight_loss_bin', 'fever_bin', 'night_sweats_bin',\n",
    "                   'smoke_lweek_bin', 'tb_prior_bin', 'tb_prior_Pul_bin', 'tb_prior_Extrapul_bin',\n",
    "                   'cough_gt14d', 'cough_gt30d']\n",
    "\n",
    "ALL_CLINICAL_FEATURES = CONTINUOUS_FEATURES + BINARY_FEATURES + country_cols\n",
    "CLINICAL_DIM = len(ALL_CLINICAL_FEATURES)\n",
    "\n",
    "print(f'\\n=== Clinical Features ({CLINICAL_DIM} total) ===')\n",
    "print(f'Continuous ({len(CONTINUOUS_FEATURES)}): {CONTINUOUS_FEATURES}')\n",
    "print(f'Binary ({len(BINARY_FEATURES)}): {BINARY_FEATURES}')\n",
    "print(f'Country ({len(country_cols)}): {country_cols}')\n",
    "\n",
    "# Check missing\n",
    "missing = meta[ALL_CLINICAL_FEATURES].isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f'\\nMissing values:')\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(f'\\nNo missing values!')\n",
    "\n",
    "# Build participant -> clinical feature vector mapping\n",
    "clinical_vectors = {}\n",
    "for _, row in meta.iterrows():\n",
    "    pid = row['participant']\n",
    "    vec = np.array([row[f] for f in ALL_CLINICAL_FEATURES], dtype=np.float32)\n",
    "    clinical_vectors[pid] = vec\n",
    "\n",
    "print(f'\\nClinical vectors built for {len(clinical_vectors)} participants')\n",
    "print(f'Vector dim: {CLINICAL_DIM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Combine Audio + Clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map clinical features to each segment (same patient = same clinical vector)\n",
    "clinical_per_segment = []\n",
    "valid_mask = []\n",
    "\n",
    "for pid in all_pids:\n",
    "    if pid in clinical_vectors:\n",
    "        clinical_per_segment.append(clinical_vectors[pid])\n",
    "        valid_mask.append(True)\n",
    "    else:\n",
    "        clinical_per_segment.append(np.zeros(CLINICAL_DIM, dtype=np.float32))\n",
    "        valid_mask.append(False)\n",
    "\n",
    "clinical_tensor = torch.tensor(np.array(clinical_per_segment), dtype=torch.float32)\n",
    "valid_mask = np.array(valid_mask)\n",
    "\n",
    "print(f'Segments with clinical data: {valid_mask.sum()}/{len(valid_mask)}')\n",
    "print(f'Missing clinical: {(~valid_mask).sum()} segments')\n",
    "\n",
    "# Filter to only segments with clinical data\n",
    "if (~valid_mask).sum() > 0:\n",
    "    audio_embeddings = audio_embeddings[valid_mask]\n",
    "    clinical_tensor = clinical_tensor[valid_mask]\n",
    "    all_labels = all_labels[valid_mask]\n",
    "    all_pids = all_pids[valid_mask]\n",
    "    print(f'After filtering: {len(audio_embeddings)} segments')\n",
    "\n",
    "COMBINED_DIM = AUDIO_DIM + CLINICAL_DIM\n",
    "print(f'\\nAudio: {AUDIO_DIM}d + Clinical: {CLINICAL_DIM}d = Combined: {COMBINED_DIM}d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Classifier + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalClassifier(nn.Module):\n",
    "    \"\"\"Two-branch architecture: audio branch + clinical branch → fusion → prediction.\"\"\"\n",
    "    def __init__(self, audio_dim, clinical_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # Audio branch: compress 1792d → 128d\n",
    "        self.audio_branch = nn.Sequential(\n",
    "            nn.LayerNorm(audio_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(audio_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        # Clinical branch: expand clinical features → 64d\n",
    "        self.clinical_branch = nn.Sequential(\n",
    "            nn.LayerNorm(clinical_dim),\n",
    "            nn.Linear(clinical_dim, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        # Fusion: 128 + 64 = 192 → prediction\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.LayerNorm(192),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(192, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, audio_emb, clinical_feat):\n",
    "        a = self.audio_branch(audio_emb)\n",
    "        c = self.clinical_branch(clinical_feat)\n",
    "        combined = torch.cat([a, c], dim=-1)  # (B, 192)\n",
    "        return self.fusion(combined).squeeze(-1)\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, audio_emb, clinical_feat, labels, pids):\n",
    "        self.audio = audio_emb\n",
    "        self.clinical = clinical_feat\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        self.pids = pids\n",
    "    def __len__(self): return len(self.audio)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'audio': self.audio[idx],\n",
    "            'clinical': self.clinical[idx],\n",
    "            'label': self.labels[idx],\n",
    "            'pid': self.pids[idx]\n",
    "        }\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LR = 5e-4\n",
    "EPOCHS = 50\n",
    "PATIENCE = 7\n",
    "N_FOLDS = 10\n",
    "\n",
    "m = MultimodalClassifier(AUDIO_DIM, CLINICAL_DIM)\n",
    "n_params = sum(p.numel() for p in m.parameters())\n",
    "print(f'=== Trial 7 Config ===')\n",
    "print(f'Audio dim: {AUDIO_DIM}, Clinical dim: {CLINICAL_DIM}')\n",
    "print(f'Architecture: Audio({AUDIO_DIM}→256→128) + Clinical({CLINICAL_DIM}→64→64) → Fusion(192→64→1)')\n",
    "print(f'Total params: {n_params:,} ({n_params/1e3:.1f}K)')\n",
    "print(f'Batch: {BATCH_SIZE}, LR: {LR}, Epochs: {EPOCHS}, Patience: {PATIENCE}')\n",
    "del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(fold_num, tr_audio, tr_clin, tr_lab, tr_pid,\n",
    "                   va_audio, va_clin, va_lab, va_pid, scaler):\n",
    "    \n",
    "    # Scale continuous features (fit on train only)\n",
    "    tr_clin_scaled = tr_clin.clone()\n",
    "    va_clin_scaled = va_clin.clone()\n",
    "    \n",
    "    # Scale continuous features (first N columns)\n",
    "    n_cont = len(CONTINUOUS_FEATURES)\n",
    "    tr_cont = tr_clin[:, :n_cont].numpy()\n",
    "    va_cont = va_clin[:, :n_cont].numpy()\n",
    "    tr_cont_scaled = scaler.fit_transform(tr_cont)\n",
    "    va_cont_scaled = scaler.transform(va_cont)\n",
    "    tr_clin_scaled[:, :n_cont] = torch.tensor(tr_cont_scaled, dtype=torch.float32)\n",
    "    va_clin_scaled[:, :n_cont] = torch.tensor(va_cont_scaled, dtype=torch.float32)\n",
    "    \n",
    "    train_ds = MultimodalDataset(tr_audio, tr_clin_scaled, tr_lab, tr_pid)\n",
    "    val_ds = MultimodalDataset(va_audio, va_clin_scaled, va_lab, va_pid)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = MultimodalClassifier(AUDIO_DIM, CLINICAL_DIM).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "    \n",
    "    best_auroc = 0\n",
    "    best_patient_logits = {}\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            audio = batch['audio'].to(DEVICE)\n",
    "            clin = batch['clinical'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(audio, clin), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        seg_probs, seg_labels, seg_pids = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                audio = batch['audio'].to(DEVICE)\n",
    "                clin = batch['clinical'].to(DEVICE)\n",
    "                probs = torch.sigmoid(model(audio, clin)).cpu().numpy()\n",
    "                seg_probs.extend(probs)\n",
    "                seg_labels.extend(batch['label'].numpy())\n",
    "                seg_pids.extend(batch['pid'])\n",
    "        \n",
    "        pt_p, pt_l = {}, {}\n",
    "        for pid, prob, lab in zip(seg_pids, seg_probs, seg_labels):\n",
    "            pt_p.setdefault(pid, []).append(prob)\n",
    "            pt_l[pid] = lab\n",
    "        \n",
    "        yt = np.array([pt_l[p] for p in pt_p])\n",
    "        yp = np.array([np.mean(v) for v in pt_p.values()])\n",
    "        auroc = roc_auc_score(yt, yp) if len(np.unique(yt)) > 1 else 0.5\n",
    "        \n",
    "        improved = ''\n",
    "        if auroc > best_auroc:\n",
    "            best_auroc = auroc\n",
    "            best_patient_logits = {pid: np.mean(v) for pid, v in pt_p.items()}\n",
    "            os.makedirs('checkpoints_t7', exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'checkpoints_t7/multimodal_fold{fold_num}.pt')\n",
    "            patience_counter = 0\n",
    "            improved = ' *'\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or improved or patience_counter >= PATIENCE:\n",
    "            print(f'  Epoch {epoch+1}/{EPOCHS} — loss: {train_loss:.4f}, AUROC: {auroc:.4f}{improved}')\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f'  Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    del model, optimizer\n",
    "    return best_auroc, best_patient_logits\n",
    "\n",
    "print('Training function ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Run 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pids = np.unique(all_pids)\n",
    "pid_labels = np.array([all_labels[all_pids == pid][0] for pid in unique_pids])\n",
    "\n",
    "n_folds_actual = min(N_FOLDS, int(min(Counter(pid_labels).values()) * 0.8))\n",
    "skf = StratifiedKFold(n_splits=n_folds_actual, shuffle=True, random_state=42)\n",
    "\n",
    "fold_aurocs = []\n",
    "all_patient_logits = {}\n",
    "all_patient_labels = {}\n",
    "\n",
    "print(f'=== TRIAL 7: Multimodal (Audio + Clinical) ===')\n",
    "print(f'Audio: Wav2Vec2 ({AUDIO_DIM}d) + Clinical ({CLINICAL_DIM}d)')\n",
    "print(f'Segments: {len(audio_embeddings)} @ 2s')\n",
    "print(f'Patients: {len(unique_pids)}')\n",
    "print(f'Folds: {n_folds_actual}, Device: {DEVICE}')\n",
    "print(f'\\n--- Benchmarks ---')\n",
    "print(f'DREAM winner (audio):    0.743')\n",
    "print(f'Trial 6 (dual audio):    0.722')\n",
    "print(f'Zambia (audio+clinical): 0.921')\n",
    "print(f'\\nStarting training...\\n')\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(unique_pids, pid_labels)):\n",
    "    print(f'=== Fold {fold+1}/{n_folds_actual} ===')\n",
    "    \n",
    "    train_pids_set = set(unique_pids[train_idx])\n",
    "    val_pids_set = set(unique_pids[val_idx])\n",
    "    \n",
    "    tr_mask = np.array([pid in train_pids_set for pid in all_pids])\n",
    "    va_mask = ~tr_mask\n",
    "    \n",
    "    tr_audio = audio_embeddings[tr_mask]\n",
    "    tr_clin = clinical_tensor[tr_mask]\n",
    "    tr_lab = all_labels[tr_mask]\n",
    "    tr_pid = all_pids[tr_mask]\n",
    "    va_audio = audio_embeddings[va_mask]\n",
    "    va_clin = clinical_tensor[va_mask]\n",
    "    va_lab = all_labels[va_mask]\n",
    "    va_pid = all_pids[va_mask]\n",
    "    \n",
    "    print(f'  Train: {len(tr_audio)} segments ({len(train_pids_set)} patients)')\n",
    "    print(f'  Val:   {len(va_audio)} segments ({len(val_pids_set)} patients)')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    auroc, patient_logits = train_one_fold(\n",
    "        fold+1, tr_audio, tr_clin, tr_lab, tr_pid,\n",
    "        va_audio, va_clin, va_lab, va_pid, scaler\n",
    "    )\n",
    "    \n",
    "    fold_aurocs.append(auroc)\n",
    "    all_patient_logits.update(patient_logits)\n",
    "    for pid in val_pids_set:\n",
    "        all_patient_labels[pid] = pid_labels[unique_pids == pid][0]\n",
    "    \n",
    "    print(f'  \\u2705 Fold {fold+1} best AUROC: {auroc:.4f}\\n')\n",
    "\n",
    "print('=' * 60)\n",
    "print(f'TRIAL 7 RESULT (Multimodal: Audio + Clinical)')\n",
    "print(f'Mean AUROC: {np.mean(fold_aurocs):.4f} +/- {np.std(fold_aurocs):.4f}')\n",
    "print(f'Per-fold: {[f\"{a:.3f}\" for a in fold_aurocs]}')\n",
    "print(f'\\n--- Full Comparison ---')\n",
    "print(f'Trial 1 (W2V audio):       0.718')\n",
    "print(f'Trial 5 (HeAR audio):      0.719')\n",
    "print(f'Trial 6 (dual audio):      0.722')\n",
    "print(f'Trial 7 (audio+clinical):  {np.mean(fold_aurocs):.4f} +/- {np.std(fold_aurocs):.4f}')\n",
    "print(f'DREAM winner (audio):      0.743')\n",
    "print(f'Zambia (audio+clinical):   0.921')\n",
    "delta = np.mean(fold_aurocs) - 0.722\n",
    "print(f'\\nDelta vs audio-only:  {delta:+.4f}')\n",
    "beat_dream = np.mean(fold_aurocs) > 0.743\n",
    "print(f'{\"\\u2705 BEAT DREAM CHALLENGE!\" if beat_dream else \"\\u26a0\\ufe0f Did not beat DREAM Challenge\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: ROC + Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_order = sorted(all_patient_logits.keys())\n",
    "y_true = np.array([all_patient_labels[p] for p in pids_order])\n",
    "y_prob = np.array([all_patient_logits[p] for p in pids_order])\n",
    "\n",
    "fpr_t7, tpr_t7, _ = roc_curve(y_true, y_prob)\n",
    "auroc_t7 = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.plot(fpr_t7, tpr_t7, 'r-', lw=2.5, label=f'Trial 7 Multimodal ({auroc_t7:.3f})')\n",
    "plt.plot([0,1],[0,1],'k--', alpha=0.2)\n",
    "plt.axhspan(0.90, 1.0, xmin=0, xmax=0.30, alpha=0.08, color='green', label='WHO TPP zone')\n",
    "plt.axhline(0.90, color='r', ls=':', alpha=0.3)\n",
    "plt.axvline(0.30, color='g', ls=':', alpha=0.3)\n",
    "plt.xlabel('FPR (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('TPR (Sensitivity)', fontsize=12)\n",
    "plt.title('RespiraHub Trial 7 \\u2014 Audio + Clinical Anamnesis', fontsize=14)\n",
    "plt.legend(fontsize=11); plt.grid(alpha=0.15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_trial7.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Thresh\":>7} {\"Sens\":>7} {\"Spec\":>7} {\"PPV\":>7} {\"NPV\":>7}')\n",
    "print('-' * 40)\n",
    "\n",
    "best_t, best_j = 0.5, -1\n",
    "who_met = False\n",
    "\n",
    "for t in np.arange(0.10, 0.90, 0.05):\n",
    "    pred = (y_prob >= t).astype(int)\n",
    "    tp = np.sum((pred == 1) & (y_true == 1))\n",
    "    tn = np.sum((pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((pred == 0) & (y_true == 1))\n",
    "    sens = tp/(tp+fn) if (tp+fn) else 0\n",
    "    spec = tn/(tn+fp) if (tn+fp) else 0\n",
    "    ppv = tp/(tp+fp) if (tp+fp) else 0\n",
    "    npv = tn/(tn+fn) if (tn+fn) else 0\n",
    "    flag = ' \\u2705 WHO' if (sens >= 0.90 and spec >= 0.70) else ''\n",
    "    if sens >= 0.90 and spec >= 0.70: who_met = True\n",
    "    j = sens + spec - 1\n",
    "    if j > best_j: best_j, best_t = j, t\n",
    "    print(f'{t:>7.2f} {sens:>7.3f} {spec:>7.3f} {ppv:>7.3f} {npv:>7.3f}{flag}')\n",
    "\n",
    "print(f'\\nBest Youden threshold: {best_t:.2f}')\n",
    "if who_met:\n",
    "    print('\\u2705 WHO TPP achievable with multimodal!')\n",
    "else:\n",
    "    print('\\u26a0\\ufe0f WHO TPP not yet met. Next: Indonesian data + domain adaptation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Clinical-Only Baseline (important for paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: how much does audio actually contribute?\n",
    "# Train same classifier but with ONLY clinical features (no audio)\n",
    "\n",
    "class ClinicalOnlyClassifier(nn.Module):\n",
    "    def __init__(self, clinical_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(clinical_dim),\n",
    "            nn.Linear(clinical_dim, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "print('=== Clinical-Only Baseline ===')\n",
    "clin_fold_aurocs = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(unique_pids, pid_labels)):\n",
    "    train_pids_set = set(unique_pids[train_idx])\n",
    "    \n",
    "    tr_mask = np.array([pid in train_pids_set for pid in all_pids])\n",
    "    va_mask = ~tr_mask\n",
    "    \n",
    "    tr_clin = clinical_tensor[tr_mask].clone()\n",
    "    va_clin = clinical_tensor[va_mask].clone()\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    n_cont = len(CONTINUOUS_FEATURES)\n",
    "    tr_clin[:, :n_cont] = torch.tensor(scaler.fit_transform(tr_clin[:, :n_cont].numpy()), dtype=torch.float32)\n",
    "    va_clin[:, :n_cont] = torch.tensor(scaler.transform(va_clin[:, :n_cont].numpy()), dtype=torch.float32)\n",
    "    \n",
    "    tr_lab = all_labels[tr_mask]\n",
    "    va_lab = all_labels[va_mask]\n",
    "    va_pid = all_pids[va_mask]\n",
    "    \n",
    "    model = ClinicalOnlyClassifier(CLINICAL_DIM).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    tr_dataset = torch.utils.data.TensorDataset(\n",
    "        tr_clin, torch.tensor(tr_lab, dtype=torch.float32))\n",
    "    tr_loader = DataLoader(tr_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    best_auroc = 0\n",
    "    patience = 0\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for batch_clin, batch_lab in tr_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(batch_clin.to(DEVICE)), batch_lab.to(DEVICE))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            va_probs = torch.sigmoid(model(va_clin.to(DEVICE))).cpu().numpy()\n",
    "        \n",
    "        # Patient-level\n",
    "        pt_p, pt_l = {}, {}\n",
    "        for pid, prob, lab in zip(va_pid, va_probs, va_lab):\n",
    "            pt_p.setdefault(pid, []).append(prob)\n",
    "            pt_l[pid] = lab\n",
    "        yt = np.array([pt_l[p] for p in pt_p])\n",
    "        yp = np.array([np.mean(v) for v in pt_p.values()])\n",
    "        auroc = roc_auc_score(yt, yp) if len(np.unique(yt)) > 1 else 0.5\n",
    "        \n",
    "        if auroc > best_auroc:\n",
    "            best_auroc = auroc\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        if patience >= 10: break\n",
    "    \n",
    "    clin_fold_aurocs.append(best_auroc)\n",
    "    del model, optimizer\n",
    "\n",
    "print(f'Clinical-only AUROC: {np.mean(clin_fold_aurocs):.4f} +/- {np.std(clin_fold_aurocs):.4f}')\n",
    "print(f'Per-fold: {[f\"{a:.3f}\" for a in clin_fold_aurocs]}')\n",
    "print(f'\\n=== Contribution Analysis ===')\n",
    "print(f'Audio-only (Trial 6):    {0.722:.4f}')\n",
    "print(f'Clinical-only:           {np.mean(clin_fold_aurocs):.4f}')\n",
    "print(f'Audio + Clinical (T7):   {np.mean(fold_aurocs):.4f}')\n",
    "print(f'\\nAudio adds:    {np.mean(fold_aurocs) - np.mean(clin_fold_aurocs):+.4f} over clinical-only')\n",
    "print(f'Clinical adds: {np.mean(fold_aurocs) - 0.722:+.4f} over audio-only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'participant': pids_order,\n",
    "    'true_label': y_true,\n",
    "    'predicted_prob': y_prob,\n",
    "})\n",
    "results_df.to_csv('patient_predictions_trial7.csv', index=False)\n",
    "\n",
    "summary = {\n",
    "    'trial': 7,\n",
    "    'approach': 'Multimodal: Wav2Vec2+HeAR audio embeddings + clinical features',\n",
    "    'audio_dim': AUDIO_DIM,\n",
    "    'clinical_dim': CLINICAL_DIM,\n",
    "    'clinical_features': ALL_CLINICAL_FEATURES,\n",
    "    'n_participants': len(pids_order),\n",
    "    'n_segments': len(audio_embeddings),\n",
    "    'n_folds': n_folds_actual,\n",
    "    'auroc_mean': round(float(np.mean(fold_aurocs)), 4),\n",
    "    'auroc_std': round(float(np.std(fold_aurocs)), 4),\n",
    "    'auroc_per_fold': [round(float(a), 4) for a in fold_aurocs],\n",
    "    'clinical_only_auroc': round(float(np.mean(clin_fold_aurocs)), 4),\n",
    "    'audio_only_auroc': 0.722,\n",
    "    'best_threshold': round(float(best_t), 2),\n",
    "    'device': str(DEVICE),\n",
    "}\n",
    "with open('training_summary_trial7.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print('Saved:')\n",
    "print('  patient_predictions_trial7.csv')\n",
    "print('  training_summary_trial7.json')\n",
    "print('  checkpoints_t7/multimodal_fold*.pt')\n",
    "print('  roc_trial7.png')\n",
    "print()\n",
    "print('=' * 60)\n",
    "print(f'TRIAL 7 COMPLETE')\n",
    "print(f'Audio-only best:  0.722')\n",
    "print(f'Clinical-only:    {np.mean(clin_fold_aurocs):.4f}')\n",
    "print(f'Multimodal:       {np.mean(fold_aurocs):.4f}')\n",
    "print(f'DREAM winner:     0.743')\n",
    "print(f'Zambia multimodal: 0.921')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
